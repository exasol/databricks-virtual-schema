package com.exasol.adapter.databricks;

import static org.hamcrest.Matchers.*;

import java.util.stream.Stream;

import org.junit.jupiter.api.*;
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.CsvSource;

import com.exasol.adapter.databricks.databricksfixture.DatabricksSchema;
import com.exasol.adapter.databricks.fixture.exasol.ExasolVirtualSchema;
import com.exasol.dbbuilder.dialects.Table;

class AdapterIT extends AbstractIntegrationTestBase {

    @Test
    void createFailsForMissingArguments() {
        assertCreateVirtualSchemaFails(null, null, containsString("F-VSDAB-1: Property 'CATALOG_NAME' is missing"));
    }

    @Test
    void createFailsForNonExistingDatabricksCatalog() {
        assertCreateVirtualSchemaFails("no-such-catalog", "schema",
                allOf(containsString("E-VSDAB-5: HTTP request for URL 'https://"),
                        containsString("Catalog 'no-such-catalog' does not exist")));
    }

    @Test
    void createFailsForNonExistingDatabricksSchema() {
        assertCreateVirtualSchemaFails("system", "no-such-schema",
                allOf(containsString("E-VSDAB-5: HTTP request for URL 'https://"),
                        containsString("Schema 'system.no-such-schema' does not exist.")));
    }

    @Test
    void schemaMetadataAvailable() {
        final ExasolVirtualSchema vs = testSetup.exasol().createVirtualSchema("system", "information_schema");
        testSetup.exasol().assertions().virtualSchemaExists(vs);
    }

    @TestFactory
    Stream<DynamicNode> dataTypeMapping() {
        // https://docs.databricks.com/en/sql/language-manual/sql-ref-datatypes.html
        return testSetup.datatypeTest() //
                .add("CHAR(10)", "CHAR(10) UTF8", 10L) //
                .add("VARCHAR(10)", "VARCHAR(10) UTF8", 10L) //
                .add("string", "VARCHAR(2000000) UTF8", 2000000L)
                .add("string not null", "VARCHAR(2000000) UTF8", 2000000L)
                .add("string generated always as ('gen')", "VARCHAR(2000000) UTF8", 2000000L)
                .add("string comment 'my column'", "VARCHAR(2000000) UTF8", 2000000L) //

                .addDecimal("TINYINT", 3, 0) //
                .addDecimal("SMALLINT", 5, 0) //
                .addDecimal("INT", 10, 0) //
                .addDecimal("BIGINT", 19, 0) //
                .addDecimal("BIGINT generated by default as identity", 19, 0) //

                .addDecimal("DECIMAL(1,0)", 1, 0) //
                .addDecimal("decimal(1,1)", 1, 1) //
                .addDecimal("DECIMAL(4,2)", 4, 2) //
                .addDecimal("DECIMAL(10)", 10, 0) //
                .addDecimal("DECIMAL(36)", 36, 0) //
                .addDecimal("DECIMAL(36,36)", 36, 36) //

                .add("FLOAT", "DOUBLE", 64) //
                .add("DOUBLE", "DOUBLE", 64) //

                .add("BOOLEAN", "BOOLEAN", 1) //

                .add("TIMESTAMP", "TIMESTAMP(3) WITH LOCAL TIME ZONE", 29) //
                .add("TIMESTAMP_NTZ", "TIMESTAMP(3)", 29) //

                .addIntervalYearToMonth("INTERVAL YEAR") //
                .addIntervalYearToMonth("INTERVAL YEAR TO MONTH") //
                .addIntervalYearToMonth("INTERVAL MONTH") //

                .addIntervalDayToSecond("INTERVAL DAY") //
                .addIntervalDayToSecond("INTERVAL DAY TO HOUR") //
                .addIntervalDayToSecond("INTERVAL DAY TO MINUTE") //
                .addIntervalDayToSecond("INTERVAL DAY TO SECOND") //
                .addIntervalDayToSecond("INTERVAL HOUR") //
                .addIntervalDayToSecond("INTERVAL HOUR TO MINUTE") //
                .addIntervalDayToSecond("INTERVAL HOUR TO SECOND") //
                .addIntervalDayToSecond("INTERVAL MINUTE") //
                .addIntervalDayToSecond("INTERVAL MINUTE TO SECOND") //
                .addIntervalDayToSecond("INTERVAL SECOND") //
                .buildTests();
    }

    @TestFactory
    Stream<DynamicNode> varcharValueConverted() {
        return testSetup.datatypeTest().addValueTest("STRING") //
                .value(null).value("", null).value("a").value("ABCxyz").value("öäüß").done() //
                .addValueTest("VARCHAR(10)") //
                .value(null).value("", null).value("a").value("1234567890").value("öäüß").done() //
                .addValueTest("VARCHAR(3000000)") //
                .value("abc").done() //
                .buildTests();
    }

    @TestFactory
    Stream<DynamicNode> charValueConverted() {
        final String longestExasolCharValue = "a".repeat(2000);
        return testSetup.datatypeTest().addValueTest("CHAR(10)") //
                .value(null).value("", null).value("a", "a         ").value("1234567890").value("öäüß      ").done() //
                .addValueTest("CHAR(3000000)").value(longestExasolCharValue).done() //
                .buildTests();
    }

    @Test
    void readingTooLongCharFails() {
        final DatabricksSchema databricksSchema = testSetup.databricks().createSchema();
        final String tooLongExasolCharValue = "a".repeat(2001);
        final Table table = databricksSchema.createTable("tab", "col", "char(3000)").insert(tooLongExasolCharValue);
        final ExasolVirtualSchema vs = testSetup.exasol().createVirtualSchema(databricksSchema);
        testSetup.exasol().assertions().assertQueryFails("select * from " + vs.qualifyTableName(table), startsWith(
                "ETL-3003: [Column=0 Row=0] [String data right truncation. String length exceeds limit of 2000 characters]"));
    }

    @ParameterizedTest
    @CsvSource(delimiterString = ";", value = { "ARRAY<INT>; ARRAY", "MAP<INT,STRING>; MAP",
            "STRUCT<id:INT,name:STRING>; STRUCT", "VARIANT; VARIANT", "BINARY; BINARY" })
    void unsupportedDataTypes(final String databricksType, final String typeInErrorMessage) {
        final DatabricksSchema databricksSchema = testSetup.databricks().createSchema();
        databricksSchema.createTable("tab", "col", databricksType + " COMMENT 'my column'");
        testSetup.exasol().assertions().assertVirtualSchemaFails(databricksSchema,
                equalTo(String.format(
                        """
                                E-VSDAB-8: Exasol does not support Databricks data type '%s' of column 'col' at position 0 with comment 'my column'

                                Mitigations:

                                * Please remove the column or change the data type.""",
                        typeInErrorMessage)));
    }

    @Test
    void unsupportedDecimalPrecision() {
        final DatabricksSchema databricksSchema = testSetup.databricks().createSchema();
        databricksSchema.createTable("tab", "col", "DECIMAL(38) COMMENT 'my column'");
        testSetup.exasol().assertions().assertVirtualSchemaFails(databricksSchema,
                equalTo("""
                        E-VSDAB-11: Unsupported decimal precision 'decimal(38,0)' for column 'col' at position 0 (comment: 'my column'), Exasol supports a maximum precision of 36.

                        Mitigations:

                        * Please remove the column or change the data type."""));
    }
}
